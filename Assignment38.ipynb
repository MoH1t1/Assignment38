{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Q1. What is the Filter method in feature selection, and how does it work?\n\nThe Filter method selects features based on their statistical properties, such as correlation or relevance to the target variable. It works by ranking the features \nusing a criterion like mutual information, chi-squared tests, or correlation, and selecting the most relevant ones before model training.\n\n# Q2. How does the Wrapper method differ from the Filter method in feature selection?\n\nThe Wrapper method evaluates feature subsets by training a model on them and assessing performance (e.g., accuracy). It is computationally expensive because\nit requires multiple model evaluations for each subset. In contrast, the Filter method uses statistical metrics independently of any model and is faster.\n\n# Q3. What are some common techniques used in Embedded feature selection methods?\n\nEmbedded methods perform feature selection during the model training process. Common techniques include:\n\nLasso Regression (L1 regularization): Forces some feature coefficients to zero.\nDecision Trees: Use feature importance derived from tree structure.\nRandom Forest: Uses feature importance metrics from ensemble learning.\nGradient Boosting: Learns feature importance while training.\n\n# Q4. What are some drawbacks of using the Filter method for feature selection?\n\nIndependence from models: It doesn't consider interactions between features and the model, which might result in suboptimal feature selection.\nNo performance feedback: It selects features based on statistical measures without considering how they affect model performance.\n    \n# Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature selection?\n\nUse the Filter method when:\nWe have a large dataset with many features.\nWe need a quick feature selection process.\nComputational resources are limited.\nWe want a less complex and less expensive method.\n\n# Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn.\n# You are unsure of which features to include in the model because the dataset contains several different\n# ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method.\n\nUsing the Filter method, I would:\nCalculate correlation coefficients between each feature and the churn label.\nUse statistical tests like chi-squared or mutual information to evaluate feature relevance.\nSelect the features that show the strongest relationship with churn while removing redundant or irrelevant features.\n    \n# Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with\n# many features, including player statistics and team rankings. Explain how you would use the Embedded\n# method to select the most relevant features for the model.\n\nIn the Embedded method, I would:\nTrain a model such as a Random Forest or Gradient Boosting on the dataset.\nUse the model’s built-in feature importance to identify the most relevant features.\nSelect the top-ranked features that contribute the most to predicting the match outcome.\n\n# Q8. You are working on a project to predict the price of a house based on its features, such as size, location,\n# and age. You have a limited number of features, and you want to ensure that you select the most important\n# ones for the model. Explain how you would use the Wrapper method to select the best set of features for the predictor.\n\nUsing the Wrapper method, I would:\nStart with a small set of features and train a model (e.g., linear regression or decision tree).\nEvaluate the model’s performance using cross-validation or a validation set.\nAdd or remove features iteratively, selecting the feature subset that maximizes model performance (e.g., lowest error or highest accuracy).",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}